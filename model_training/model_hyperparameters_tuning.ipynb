{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4712f9ae",
   "metadata": {},
   "source": [
    "# Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv3D,MaxPooling3D, Dense,Flatten, Concatenate, ConvLSTM2D, ConvLSTM3D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef5060",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_1step(all_sites_df, fh_step = 1, \n",
    "               extra_features :list = []):\n",
    "\n",
    "\n",
    "    # morning model\n",
    "\n",
    "    morn_cols = ['site','Datetime', f'I_lead_{fh_step}step', 'I', f'I_lead_{fh_step}step_back1D', f'hour_index_lead_{fh_step}step', f'iclr_lead_{fh_step}step', 'ci_center', f'ci_est(t+{fh_step})']\n",
    "\n",
    "    _df_morn = all_sites_df[all_sites_df['Datetime'].dt.time.isin(pd.date_range('06:30:00', '08:30:00', freq='30min').time)][morn_cols]\n",
    "\n",
    "    \n",
    "    _df_morn = _df_morn.dropna()\n",
    "    date_morn_index = _df_morn['Datetime'].dt.date\n",
    "\n",
    "    train_date_morn_cond = date_morn_index.isin(train_date_list)\n",
    "    end_date_morn_cond = date_morn_index.isin(test_date_list)\n",
    "\n",
    "    _df_morn_train = _df_morn[train_date_morn_cond].set_index(['site', 'Datetime'])\n",
    "    _df_morn_test = _df_morn[end_date_morn_cond].set_index(['site', 'Datetime'])\n",
    "\n",
    "\n",
    "    X_train_morn, X_test_morn = _df_morn_train.drop(columns=[f'I_lead_{fh_step}step']), _df_morn_test.drop(columns=[f'I_lead_{fh_step}step'])\n",
    "    y_train_morn, y_test_morn = _df_morn_train[f'I_lead_{fh_step}step'], _df_morn_test[f'I_lead_{fh_step}step']\n",
    "\n",
    "    # noon model\n",
    "\n",
    "    noon_cols = ['site','Datetime',  f'I_lead_{fh_step}step', 'I', f'I_lead_{fh_step}step_back1D', f'hour_index_lead_{fh_step}step', f'iclr_lead_{fh_step}step', \n",
    "                 'I_lag_1step', 'I_lag_2step', 'I_lag_3step', 'I_lag_4step','I_lag_5step', 'ci_center', f'ci_est(t+{fh_step})']\n",
    "\n",
    "    _df_noon = all_sites_df[all_sites_df['Datetime'].dt.time.isin(pd.date_range('09:00:00', '14:30:00', freq='30min').time)][noon_cols]\n",
    "    _df_noon = _df_noon.dropna()\n",
    "    date_noon_index = _df_noon['Datetime'].dt.date\n",
    "\n",
    "    train_date_noon_cond = date_noon_index.isin(train_date_list)\n",
    "    end_date_noon_cond = date_noon_index.isin(test_date_list)\n",
    "\n",
    "    _df_noon_train = _df_noon[train_date_noon_cond].set_index(['site', 'Datetime'])\n",
    "    _df_noon_test = _df_noon[end_date_noon_cond].set_index(['site', 'Datetime'])\n",
    "\n",
    "\n",
    "    X_train_noon, X_test_noon = _df_noon_train.drop(columns=[f'I_lead_{fh_step}step']), _df_noon_test.drop(columns=[f'I_lead_{fh_step}step'])\n",
    "    y_train_noon, y_test_noon = _df_noon_train[f'I_lead_{fh_step}step'], _df_noon_test[f'I_lead_{fh_step}step']\n",
    "\n",
    "    # evening model \n",
    "    even_cols = morn_cols\n",
    "    _df_even = all_sites_df[all_sites_df['Datetime'].dt.time.isin(pd.date_range('15:00:00', '17:00:00', freq='30min').time)][even_cols]\n",
    "\n",
    "    \n",
    "    _df_even = _df_even.dropna()\n",
    "    date_even_index = _df_even['Datetime'].dt.date\n",
    "\n",
    "    train_date_even_cond = date_even_index.isin(train_date_list)\n",
    "    end_date_even_cond = date_even_index.isin(test_date_list)\n",
    "\n",
    "    _df_even_train = _df_even[train_date_even_cond].set_index(['site', 'Datetime'])\n",
    "    _df_even_test = _df_even[end_date_even_cond].set_index(['site', 'Datetime'])\n",
    "\n",
    "\n",
    "    X_train_even, X_test_even = _df_even_train.drop(columns=[f'I_lead_{fh_step}step']), _df_even_test.drop(columns=[f'I_lead_{fh_step}step'])\n",
    "    y_train_even, y_test_even = _df_even_train[f'I_lead_{fh_step}step'], _df_even_test[f'I_lead_{fh_step}step']\n",
    "\n",
    "\n",
    "    return X_train_morn, X_test_morn, y_train_morn, y_test_morn, X_train_noon, X_test_noon,y_train_noon, y_test_noon, X_train_even, X_test_even, y_train_even, y_test_even\n",
    "def split_val_from_train(X_train, y_train) :\n",
    "  _X = X_train.reset_index()\n",
    "  _y = y_train.reset_index()\n",
    "\n",
    "  date_index = _X['Datetime'].dt.date\n",
    "\n",
    "  train_cond = date_index.isin(train_date_val_list)\n",
    "  val_cond = date_index.isin(val_date_list)\n",
    "\n",
    "  _X_train_val, _y_train_val = _X[train_cond].set_index(['site', 'Datetime']), _y[train_cond].set_index(['site', 'Datetime'])\n",
    "  _X_val, _y_val = _X[val_cond].set_index(['site', 'Datetime']), _y[val_cond].set_index(['site', 'Datetime'])\n",
    "\n",
    "  return _X_train_val, _y_train_val, _X_val, _y_val\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "def split_to_each_step(df, fh_step):\n",
    "  use_cols = ['site','Datetime', f'I_lead_{fh_step}step', \n",
    "                     'I', f'I_lead_{fh_step}step_back1D', f'hour_index_lead_{fh_step}step', f'iclr_lead_{fh_step}step', \n",
    "                     'I_lag_1step', 'ci_center', f'ci_est(t+{fh_step})']\n",
    "\n",
    "\n",
    "  _df = df[use_cols]\n",
    "  _df = _df.dropna()\n",
    "  _df['Datetime'] = pd.to_datetime(_df['Datetime'])\n",
    "\n",
    "  date_index = _df['Datetime'].dt.date\n",
    "  _df_train = _df[date_index.isin(train_date_val_list)]\n",
    "  _df_val = _df[date_index.isin(val_date_list)]\n",
    "  _df_test = _df[date_index.isin(test_date_list)]\n",
    "\n",
    "  \n",
    "  _df_train = _df_train.set_index(['site', 'Datetime'])\n",
    "  _df_val = _df_val.set_index(['site', 'Datetime'])\n",
    "  _df_test = _df_test.set_index(['site', 'Datetime'])\n",
    "\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_train = _df_train.drop(columns=[f'I_lead_{fh_step}step'])\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "  _df_train[list(set(_df_train.columns) - set([f'I_lead_{fh_step}step']))] = X_train\n",
    "  _df_val[list(set(_df_val.columns) - set(['filename', f'I_lead_{fh_step}step']))] = scaler.transform(_df_val.drop(columns=['filename', f'I_lead_{fh_step}step']))\n",
    "  _df_test[list(set(_df_test.columns) - set([f'I_lead_{fh_step}step']))] = scaler.transform(_df_test.drop(columns=[f'I_lead_{fh_step}step']))\n",
    "  \n",
    "  return _df_train, _df_val, _df_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26aa574",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b8123",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_df = pd.read_csv('processed_all_sites_HS1e4_df_not_imputed_R_channel.csv', parse_dates = ['Datetime'])\n",
    "all_sites_df = all_sites_df.iloc[:, 1:]\n",
    "all_sites_df['site'] = all_sites_df['site'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfc9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_date_list, test_date_list, val_date_list,train_date_val_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_morn, X_test_morn, y_train_morn, y_test_morn, X_train_noon, X_test_noon, y_train_noon, y_test_noon, X_train_even, X_test_even, y_train_even, y_test_even = split_data_1step(all_sites_df)\n",
    "X_train_val_noon, y_train_val_noon, X_val_noon, y_val_noon = split_val_from_train(X_train_noon, y_train_noon)\n",
    "scaler = StandardScaler()\n",
    "X_train_val_noon_scaled = scaler.fit_transform(X_train_val_noon)\n",
    "X_val_noon_scaled = scaler.transform(X_val_noon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ba724",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning\n",
    "- We use WandB sweep feature to log hyperpameters tuning of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46925cd",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e08a947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatanon-t\u001b[0m (\u001b[33mduo-y4\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'validation_loss'},\n",
      " 'parameters': {'C': {'values': [10, 100, 200, 400]},\n",
      "                'epsilon': {'values': [0.1, 1, 10]},\n",
      "                'gamma': {'values': [0.01, 0.1]},\n",
      "                'kernel': {'values': ['rbf']}}}\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "import wandb\n",
    "key = ''\n",
    "wandb.login(key=key)\n",
    "\n",
    "sweep_config = {'method': 'grid'}\n",
    "# choose hyperparameter choice \n",
    "parameters_dict = {\n",
    "    'C' : {\n",
    "        'values' : [10, 100, 200, 400]\n",
    "        },\n",
    "    'gamma' : {\n",
    "        'values' : [0.01, 0.1]\n",
    "    },\n",
    "    'epsilon' : {\n",
    "        'values' : [0.1, 1, 10]\n",
    "        },\n",
    "    'kernel' : {\n",
    "        'values' : ['rbf']\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "metric = {\n",
    "    'name': 'validation_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all function to train model\n",
    "def SVR_model(C = 1, gamma = 1, epsilon = 1, kernel = 'rbf'):\n",
    "\n",
    "  model = SVR(C = C, gamma = gamma, epsilon = epsilon,kernel = kernel)\n",
    "  return model\n",
    "\n",
    "def train(model):  \n",
    "\n",
    "    model.fit(X_train_val_noon_scaled, np.array(y_train_val_noon).ravel()) \n",
    "\n",
    "    y_pred_train = model.predict(X_train_val_noon_scaled)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "\n",
    "    y_pred_val = model.predict(X_val_noon_scaled)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    train_rmse = (mean_squared_error(y_train_val_noon, y_pred_train))** 0.5\n",
    "    train_mae = mean_absolute_error(y_train_val_noon, y_pred_train)\n",
    "\n",
    "    val_rmse = (mean_squared_error(y_val_noon, y_pred_val))** 0.5\n",
    "    val_mae = mean_absolute_error(y_val_noon, y_pred_val)\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae, \n",
    "        'val_rmse': val_rmse, \n",
    "        'val_mae': val_mae\n",
    "      })\n",
    "\n",
    "    \n",
    "def sweep_train(config_defaults=None):\n",
    "    # Initialize wandb with a sample project name\n",
    "    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n",
    "        wandb.config.architecture_name = \"SVR\"\n",
    "        wandb.config.dataset_name = \"validation\"\n",
    "        # initialize model\n",
    "        model = SVR_model(C = wandb.config.C, \n",
    "                   gamma = wandb.config.gamma, \n",
    "                   epsilon = wandb.config.epsilon,\n",
    "                   kernel = wandb.config.kernel\n",
    "                   )\n",
    "        train(model)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"HyperTune_SVR_noon_latest\")\n",
    "wandb.agent(sweep_id, function=sweep_train)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3d2a7",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a085460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "key = ''\n",
    "wandb.login(key=key)\n",
    "\n",
    "sweep_config = {'method': 'grid'}\n",
    "\n",
    "# choose hyperparameter choice \n",
    "parameters_dict = {\n",
    "    'n_estimators' : {\n",
    "        'values' : [100, 500, 1000, 1500, 2000]\n",
    "        },\n",
    "    'max_depth' : {\n",
    "        'values' : [30, 35, 40, 45, 50]\n",
    "    },\n",
    "    'min_samples_leaf' : {\n",
    "        'values' : [25]\n",
    "        },\n",
    "    'min_samples_split' : {\n",
    "        'values' : [25]\n",
    "    \n",
    "    }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# \n",
    "metric = {\n",
    "    'name': 'validation_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all function to train model\n",
    "def RF(max_depth=10, min_samples_leaf= 25, min_samples_split = 25, n_estimators=1000, random_state=42):\n",
    "\n",
    "  model = RandomForestRegressor(max_depth=max_depth, min_samples_leaf= min_samples_leaf, min_samples_split = min_samples_split, n_estimators=n_estimators, random_state=42)\n",
    "  return model\n",
    "\n",
    "def train(model):  \n",
    "    model.fit(X_train_val_noon, y_train_val_noon) \n",
    "\n",
    "    y_pred_train = model.predict(X_train_val_noon)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "\n",
    "    y_pred_val = model.predict(X_val_noon)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    train_rmse = (mean_squared_error(y_train_val_noon, y_pred_train))** 0.5\n",
    "    train_mae = mean_absolute_error(y_train_val_noon, y_pred_train)\n",
    "\n",
    "    val_rmse = (mean_squared_error(y_val_noon, y_pred_val))** 0.5\n",
    "    val_mae = mean_absolute_error(y_val_noon, y_pred_val)\n",
    "\n",
    "    wandb.log({\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae, \n",
    "        'val_rmse': val_rmse, \n",
    "        'val_mae': val_mae\n",
    "      })\n",
    "\n",
    "    \n",
    "def sweep_train(config_defaults=None):\n",
    "    # Initialize wandb with a sample project name\n",
    "    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n",
    "\n",
    "        wandb.config.architecture_name = \"RF\"\n",
    "        wandb.config.dataset_name = \"validationr\"\n",
    "\n",
    "        # initialize model\n",
    "        model = RF(max_depth = wandb.config.max_depth, \n",
    "                   min_samples_leaf = wandb.config.min_samples_leaf, \n",
    "                   min_samples_split = wandb.config.min_samples_split, \n",
    "                   n_estimators = wandb.config.n_estimators\n",
    "                   )\n",
    "        train(model)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"HyperTune_RF_noon_latest\")\n",
    "wandb.agent(sweep_id, function=sweep_train)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e96ce",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose hyperparameter choice \n",
    "sweep_config = {'method': 'grid'}\n",
    "parameters_dict = {\n",
    "    'boosting_type': {\n",
    "        'values' : ['gbdt','goss'] },\n",
    "    'n_estimators' : {\n",
    "        'values' : [500, 1000, 1500,2000] },\n",
    "    'max_depth' : {\n",
    "        'values' : [5,10 ,15,20]},\n",
    "    'learning_rate' : {\n",
    "        'values' : [0.005, 0.01, 0.05, 0.1] },\n",
    "    'max_bin' :{\n",
    "        'values' : [256, 512] },\n",
    "    'reg_alpha': {\n",
    "        'values': [0,1e-1, 1] },\n",
    "\n",
    "#     'reg_lambda': {\n",
    "#         'values' : [0, 1e-1, 1] },        \n",
    "    }\n",
    "    \n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# \n",
    "metric = {\n",
    "    'name': 'validation_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all function to train model\n",
    "from lightgbm import LGBMRegressor\n",
    "def lgbm(boosting_type = 'gbdt', learning_rate = 0.1, \n",
    "     max_bin=256, max_depth = 5, n_estimators=100, reg_alpha=0):\n",
    "\n",
    "  model = LGBMRegressor(boosting_type = boosting_type, learning_rate = learning_rate, \n",
    "                             max_bin=max_bin, max_depth = max_depth, n_estimators=n_estimators, \n",
    "                            reg_alpha=reg_alpha)\n",
    "  return model\n",
    "\n",
    "def train(model):  \n",
    "\n",
    "    \n",
    "    model.fit(X_train_val_noon, y_train_val_noon) \n",
    "\n",
    "    y_pred_train = model.predict(X_train_val_noon)\n",
    "    y_pred_train[y_pred_train < 0] = 0\n",
    "\n",
    "    y_pred_val = model.predict(X_val_noon)\n",
    "    y_pred_val[y_pred_val < 0] = 0\n",
    "\n",
    "    train_rmse = (mean_squared_error(y_train_val_noon, y_pred_train))** 0.5\n",
    "    train_mae = mean_absolute_error(y_train_val_noon, y_pred_train)\n",
    "\n",
    "    val_rmse = (mean_squared_error(y_val_noon, y_pred_val))** 0.5\n",
    "    val_mae = mean_absolute_error(y_val_noon, y_pred_val)\n",
    "\n",
    "    wandb.log({\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae, \n",
    "        'val_rmse': val_rmse, \n",
    "        'val_mae': val_mae\n",
    "      })\n",
    "\n",
    "def sweep_train(config_defaults=None):\n",
    "    # Initialize wandb with a sample project name\n",
    "    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n",
    "        wandb.config.architecture_name = \"LightGBM\"\n",
    "\n",
    "        # initialize model\n",
    "        model = lgbm(boosting_type = wandb.config.boosting_type, learning_rate = wandb.config.learning_rate, \n",
    "                             max_bin=wandb.config.max_bin, max_depth = wandb.config.max_depth, \n",
    "                             n_estimators=wandb.config.n_estimators, \n",
    "                            reg_alpha=wandb.config.reg_alpha)\n",
    "        train(model)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"HypeTune_lgbm_noon\")\n",
    "wandb.agent(sweep_id, function=sweep_train)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8887c2",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd78488",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_step=1\n",
    "df_train, df_val, df_test, scaler = split_to_each_step(all_sites_df, fh_step=fh_step)\n",
    "print(f'model {fh_step} step, the data have {df_train.shape[0]} samples for training and {df_val.shape[0]} for validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop(columns=[f'I_lead_{fh_step}step']),   df_train[f'I_lead_{fh_step}step']\n",
    "X_val, y_val = df_val.drop(columns=[f'I_lead_{fh_step}step']),   df_test[f'I_lead_{fh_step}step']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "key = ''\n",
    "wandb.login(key=key)\n",
    "\n",
    "sweep_config = {'method': 'grid'}\n",
    "\n",
    "# choose hyperparameter choice \n",
    "parameters_dict = {\n",
    "    'learning_rate' : {\n",
    "        'values' : [0.005, 0.01, 0.05]\n",
    "    },\n",
    "    'no_layers' : {\n",
    "        'values' : [1, 2, 3, 4, 5]\n",
    "        },\n",
    "    'dense' : {\n",
    "        'values' : [32, 64]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# \n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96622ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all function to train model\n",
    "from keras import backend as K\n",
    "def MLP(nb_features=10, no_layers = 3, dense = 32):\n",
    "  K.clear_session()\n",
    "  model = Sequential()\n",
    "  model.add(tf.keras.Input(shape=(nb_features,)))\n",
    "\n",
    "\n",
    "  for i in range(no_layers):\n",
    "    model.add(Dense(dense, activation='relu'))\n",
    "\n",
    "  model.add(Dense(1,activation='relu'))\n",
    "  model.compile(loss=\"mean_absolute_error\", optimizer='adam',\n",
    "                metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "  return model\n",
    "\n",
    "\n",
    "def train(model, batch_size=64, epochs = 30, lr=1e-3, optimizer='adam'):  \n",
    "    \n",
    "    # Compile model like you usually do.\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.compile(loss=\"mean_absolute_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    # callback setup\n",
    "    cbs = [WandbCallback()]\n",
    "\n",
    "    model.fit(X_train, \n",
    "              y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs,\n",
    "              validation_data=(X_val, y_val), \n",
    "              # learning_rate = lr,\n",
    "              callbacks=cbs)\n",
    "    \n",
    "def sweep_train(config_defaults=None):\n",
    "    # Initialize wandb with a sample project name\n",
    "    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n",
    "        wandb.config.architecture_name = \"ANN\"\n",
    "        wandb.config.dataset_name = \"validation\"\n",
    "        # initialize model\n",
    "        model = MLP(nb_features = X_train.shape[1], no_layers = wandb.config.no_layers, dense = wandb.config.dense)\n",
    "        train(model, \n",
    "              # wandb.config.batch_size,\n",
    "              epochs = 30,\n",
    "              lr= wandb.config.learning_rate\n",
    "              )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
